#!/usr/bin/env python
# -*- coding: utf-8 -*-
# -------------------------------------------------------------------
# C4 â€“ Exploiting TransH embeddings
#   â€¢ Unsupervised: K-Means clustering of papers  (elbow â†’ best k)
#   â€¢ Supervised : Citation-link classifier (logistic regression)
# -------------------------------------------------------------------

import os, json
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import (
    silhouette_score,
    roc_auc_score,
    precision_recall_fscore_support,
    accuracy_score,
    roc_curve,
)
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
import torch


# -------------------------------------------------------------------
# 0. Paths & constants
# -------------------------------------------------------------------
ROOT        = Path(__file__).resolve().parents[1]
DATA_DIR    = ROOT / "data"
experiments_dir = Path(__file__).resolve().parent.parent / "experiments"
FIG_DIR     = ROOT / "figures"
SEED_PAPER  = "<http://example.org/inst/Paper_0>"
print("RUTA: ",DATA_DIR)

MODEL_DIR   = DATA_DIR / "TransH_model"          # model chosen in C3
CHK_PATH_PT = MODEL_DIR / "best_model.pt"
CHK_PATH_PK = MODEL_DIR / "trained_model.pkl"

os.makedirs(FIG_DIR, exist_ok=True)
rng = np.random.default_rng(42)

# -------------------------------------------------------------------
# 1. Load TransH embeddings & helper dicts
# -------------------------------------------------------------------
# 1. Load TransH embeddings
print("Loading TransH checkpoint â€¦")
if CHK_PATH_PT.exists():
    from pykeen.models import Model
    model = Model.from_checkpoint(CHK_PATH_PT)
elif CHK_PATH_PK.exists():
    model = torch.load(CHK_PATH_PK, map_location="cpu", weights_only=False)
else:
    raise FileNotFoundError("No checkpoint found")

# recover entity-id mapping
try:
    ent_to_id = model.triples_factory.entity_to_id
except AttributeError:
    print("  â€¢ triples_factory not found; loading train.tsv")
    from pykeen.triples import TriplesFactory
    tf_train = TriplesFactory.from_path(DATA_DIR / "train.tsv")
    ent_to_id = tf_train.entity_to_id

id_to_ent = {v: k for k, v in ent_to_id.items()}


# full entity matrix â†’ numpy
emb_matrix = model.entity_representations[0]().detach().cpu().numpy()

# restrict to papers only
paper_mask = np.fromiter(
    (uri.startswith("<http://example.org/inst/Paper_") for uri in id_to_ent.values()),
    dtype=bool,
    count=len(id_to_ent),
)
paper_vecs = emb_matrix[paper_mask]
paper_uris = np.array(list(id_to_ent.values()))[paper_mask]

uri_to_idx = {u: i for i, u in enumerate(paper_uris)}

# titles (quick scan)
titles_dict = {}
RAW_PATH = experiments_dir / "triples_raw.tsv"
with RAW_PATH.open(encoding="utf-8") as fh:
    for line in fh:
        h, r, t = line.rstrip("\n").split("\t")
        
        if r == '<http://example.org/res/title>':
            titles_dict[h] = t.strip('"')

# -------------------------------------------------------------------
# 2. Unsupervised exploitation â€“ K-Means clustering
# -------------------------------------------------------------------
print("\nðŸ—‚  Unsupervised: elbow search for k")
inertias = []
K_RANGE  = range(2, 16)
for k in K_RANGE:
    km = KMeans(n_clusters=k, random_state=42, n_init="auto")
    km.fit(paper_vecs)
    inertias.append(km.inertia_)
plt.figure(figsize=(7, 4))
plt.plot(list(K_RANGE), inertias, marker="o")
plt.xlabel("k")
plt.ylabel("Inertia")
plt.title("Elbow plot")
plt.tight_layout()
plt.savefig(FIG_DIR / "c4_elbow.png")
plt.close()

# choose k at elbow â€“ first point where relative drop < 5 %
pct_drop = np.abs(np.diff(inertias) / inertias[:-1])
elbow_k = next((K_RANGE[i] for i, p in enumerate(pct_drop) if p < 0.05), 8)
print(f"   â†³ selected k = {elbow_k}")

kmeans = KMeans(n_clusters=elbow_k, random_state=42, n_init="auto")
labels = kmeans.fit_predict(paper_vecs)
sil    = silhouette_score(paper_vecs, labels)
print(f"   silhouette = {sil:.3f}")

pd.DataFrame({"uri": paper_uris, "cluster": labels}) \
  .to_csv(DATA_DIR / "c4_clusters.csv", index=False)

# -------------------------------------------------------------------
# 3. Visualisation (PCA and t-SNE)
# -------------------------------------------------------------------
def plot_2d(coords, title_tag):
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=coords[:, 0], y=coords[:, 1], hue=labels,
                    palette="tab20", s=8, linewidth=0, legend=False)
    # seed paper
    plt.scatter(*coords[uri_to_idx[SEED_PAPER]], marker="*", s=180,
                color="black", label="Seed Paper 0")
    # top-5 recommendations
    top5 = pd.read_csv(DATA_DIR / "predictions_top5_TransH.csv").tail_label
    for rank, uri in enumerate(top5, 1):
        idx = uri_to_idx.get(uri)
        if idx is None: continue
        plt.scatter(*coords[idx], marker="^", s=90, color="red")
        plt.text(coords[idx, 0], coords[idx, 1], str(rank),
                 color="red", fontsize=8, weight="bold")
    plt.title(title_tag)
    plt.tight_layout()
    ftag = title_tag.lower().replace(" ", "_")
    plt.savefig(FIG_DIR / f"c4_{ftag}.png")
    plt.close()

pca_coords = PCA(n_components=2, random_state=42).fit_transform(paper_vecs)
plot_2d(pca_coords, f"PCA (k={elbow_k}, sil={sil:.3f})")

tsne_coords = TSNE(n_components=2, perplexity=30, n_iter=1000,
                   random_state=42).fit_transform(paper_vecs)
plot_2d(tsne_coords, "t-SNE")

# -------------------------------------------------------------------
# 4. Supervised exploitation â€“ citation-link classifier
# -------------------------------------------------------------------
print("\n Supervised: citation-edge prediction")

from pykeen.triples import TriplesFactory
# Load test triples for positive citation pairs
tf_test = TriplesFactory.from_path(DATA_DIR / "test.tsv")

pos_pairs = {
    (h, t)
    for h, r, t in tf_test.triples
    if (
        r == '<http://example.org/res/cites>'
        and h in uri_to_idx          # ensure both endpoints have embeddings
        and t in uri_to_idx
    )
}


neg_pairs = set()
while len(neg_pairs) < len(pos_pairs):
    a, b = rng.choice(paper_uris, 2, replace=False)
    if (a, b) not in pos_pairs:
        neg_pairs.add((a, b))

def hadamard(u, v):
    return paper_vecs[uri_to_idx[u]] * paper_vecs[uri_to_idx[v]]

X_pos = np.vstack([hadamard(u, v) for u, v in pos_pairs])
X_neg = np.vstack([hadamard(u, v) for u, v in neg_pairs])
y     = np.hstack([np.ones(len(X_pos)), np.zeros(len(X_neg))])
X     = np.vstack([X_pos, X_neg])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)

clf = LogisticRegression(max_iter=200)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]

prec, rec, f1, _ = precision_recall_fscore_support(
    y_test, y_pred, average="binary", zero_division=0)
acc  = accuracy_score(y_test, y_pred)
auc  = roc_auc_score(y_test, y_prob)

print(f"   accuracy   = {acc:.3f}")
print(f"   precision  = {prec:.3f}")
print(f"   recall     = {rec:.3f}")
print(f"   F1-score   = {f1:.3f}")
print(f"   ROC-AUC    = {auc:.3f}")

fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f"AUC = {auc:.3f}")
plt.plot([0, 1], [0, 1], "--", color="grey")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Citation-link classifier ROC")
plt.tight_layout()
plt.savefig(FIG_DIR / "c4_roc.png")
plt.close()

json.dump({"accuracy": acc, "precision": prec, "recall": rec,
           "f1": f1, "roc_auc": auc},
          (DATA_DIR / "c4_supervised_metrics.json").open("w"), indent=2)

print("\nâœ“ C4 exploitation complete plots in /figures, results in /data.")
